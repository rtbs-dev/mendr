---
jupyter:
  jupytext:
    formats: ipynb,qmd,md:myst
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: mendr
    language: python
    name: mendr
---

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
import numpy as np
import mendr.metrics as m
from scipy.stats import ecdf
from scipy.integrate import trapezoid, simpson, cumulative_trapezoid
import sparse
import time
import matplotlib.pyplot as plt
import seaborn as sns
# import yappi
%matplotlib inline
# import seaborn as sns
```

```{python}
rng = np.random.default_rng(2)

X = rng.choice(rng.beta(0.3, 0.7, size=10), size=25)
y = rng.binomial(1,X).astype(bool)

Y = m.Contingent.from_scalar(y, X)
T = Y.weights

# T = m.gen_thres_vals(X)
# E = ecdf(X)
# T = np.pad(E.cdf.probabilities, ((1,1)),constant_values=(0,1))
# Xt = m._all_thres(E.cdf.evaluate(X), T).astype(bool)

# Yt = m.Contingent(y, m._all_thres(X, np.unique(X)))

# d = percentileofscore(X, T)/100.
# d = ecdf(X).sf.evaluate(T)
# d = ecdf(X).cdf.evaluate(T)
# Y
```

```{python}
plt.plot(T)
```

```{python}
plt.hist(X);
```

```{python}
np.round(ecdf(X).cdf.evaluate(X),2)
```

```{python}
# X.toarray().sum(axis=0).mean()
# t ~ O(k*a*b)
```

```{python}
y.astype(int)
```

```{python}
# plt.hist(X, density=True)
# plt.spy(Xt)
# Xt.astype(int)
Y.F
```

```{python}
plt.plot(T,Y.F)
# plt.plot(d,Y.F)
plt.axvline(1-y.mean(), color='k', ls='--')
simpson(m.F1(Y), x=T), m.F1(Y).mean()#, -simpson(m.F1(Y), x=d)
```

```{python}
plt.plot(T,Y.mcc)
# plt.plot(d,Y.mcc)
plt.axvline(1-y.mean(), color='k', ls='--')
simpson(m.matthews_corrcoef(Y), x=T), m.matthews_corrcoef(Y).mean()#, -simpson(m.matthews_corrcoef(Y), x=d)
```

```{python}
plt.plot(T,m.fowlkes_mallows(Y))

# plt.plot(d,m.fowlkes_mallows(Y))
plt.axvline(1-y.mean(), color='k', ls='--')
trapezoid(m.fowlkes_mallows(Y), x=T), m.fowlkes_mallows(Y).mean(), #-simpson(m.fowlkes_mallows(Y), x=d)
```

```{python}
plt.figure(figsize=(5,5))
plt.step(Y.recall, Y.precision, color='k', ls='--', where='post')

def contour_Fscore():
    f_scores = np.linspace(0.2, 0.8, num=4)
    lines, labels = [], []
    for f_score in f_scores:
        x = np.linspace(0.0001, 1.)

        y_f1 = f_score * x / (2 * x - f_score)
        y_fm = f_score**2/x

        (l,) = plt.plot(x[0<=y_f1], y_f1[0<=y_f1], color="xkcd:orange", alpha=0.2)
        (l,) = plt.plot(x[0<=y_fm], y_fm[y_fm >= 0], color="xkcd:sky", alpha=0.2)
        # midpt = y_fm[25]-0.03
        plt.annotate("fm={0:0.1f}".format(f_score), xy=(y_f1[25]+0.01, y_f1[25]+0.1), color='xkcd:slate')
        # plt.annotate("f1={0:0.1f}".format(f_score), xy=(1.1, y_f1[48]-0.01), color='xkcd:orange')


contour_Fscore()

# der contour_
plt.plot(Y.recall[::-1], cumulative_trapezoid(Y.precision[::-1], x=Y.recall[::-1], initial=0.))
plt.plot(Y.recall[::-1], np.cumsum(np.diff(Y.recall[::-1], prepend=0)*Y.precision[::-1]))

plt.ylim(0,1.1)
plt.xlim(0,1.1)
sns.despine()
# np.cumsum(Y.precision[::-1]*np.diff(Y.recall[::-1], prepend=0))
# Y.recall
```

```{python}
plt.plot(np.cumsum(np.diff(Y.recall[::-1], prepend=0)*Y.precision[::-1]))
```

```{python}
plt.plot(Y.recall,cumulative_trapezoid(Y.precision, x=Y.recall, initial=0,))
```

```{python}
from affinis.proximity import sinkhorn
from affinis.priors import pseudocount
from affinis.plots import hinton

from affinis import associations as asc
from affinis.associations import (
    _pursue_tree_basis, 
    _spanning_forests_obs_bootstrap, 
    forest_pursuit_edge, 
    forest_pursuit_interaction,
    forest_pursuit_cts,
    expected_forest_maximization,
)
from affinis.utils import groupby_col0, sparse_adj_to_incidence, _norm_diag, edge_weights_to_laplacian
from affinis.distance import adjusted_forest_dists, bilinear_dists, generalized_graph_dists

from mendr.experiments import (
    load_graph, _datasets,_estimators, _metrics,_sq, _dataset_paths
)
from mendr.metrics import minmax_tf
from dvc.api import DVCFileSystem
from scipy.special import expit
from scipy.stats import zscore, norm, beta, iqr
from scipy.sparse import csr_array, coo_array
from sklearn.metrics import average_precision_score
import re
from itertools import chain
from tqdm.auto import tqdm
import pandas as pd
import awkward as ak

from sklearn.covariance import graphical_lasso, GraphicalLassoCV
# !cd /home/rtbsexton/Documents/code/graph-recovery-dataset/docs
# DVCFileSystem().find("data/", dvc_only=True, detail=False)
# exp = load_graph( list(_datasets.keys())[-1])
eg_gr = list(_datasets)[210]
# eg_gr = list(_datasets)[80]

print(eg_gr)
exp = load_graph(eg_gr)
# np.exp(-asc.mutual_information(csr_array(exp.activations.to_array().to_scipy_sparse())))
# exp.activations.to_array().sum(axis=1).todense()
# [exp.jumps.shape, exp.graph.to_array()
# _datasets
# _dataset_paths
# _estimators
# [load_graph(i) for i in list(_datasets.keys())[:10]] 
X = csr_array(exp.activations.to_array().to_scipy_sparse())
gT = _sq(exp.graph.to_array().todense()).astype(bool)
X.sum(axis=1).mean()**2/X.shape[1]**2
# X.shape
```

```{python}
X.toarray()
```

```{python}
# exp.activations.to_array().sum(axis=1).mean(), exp.jumps.shape[1]
# X.todense()
glcv = GraphicalLassoCV(assume_centered=True).fit(X.toarray())
```

```{python}
M = m.Contingent.from_scalar(gT,_sq(-glcv.get_precision()))

plt.plot(M.weights, M.mcc, color='k', alpha=0.3)
```

```{python}
# plt.imshow(sinkhorn(X).todense())
# w,n=X.shape
# BA = np.block(
#     [[np.zeros((n,n)),X.todense().T],
#      [X.todense(),np.zeros((w,w))]]
# )
# Snk = sinkhorn(BA)
# hinton((Snk.T@Snk)[:n,:n])
```

```{python}
# pseudocount(0.5)(X, X.sum(axis=0))

# np.dot(X.sum(axis=0), X.sum(axis=1))
# (X.T@X).todense()/np.multiply.outer(X.sum(axis=0), np.ones(X.shape[1]))
# D = np.diag(1/X.sum(axis=0))
# (1-D@(X.T) == (X/X.sum(axis=0)).T).sum()

# list(_datasets)[200]
# BA.shape
# _estimators['TS'](X)

# _pursue_tree_basis(
csr_array(sparse.COO.from_scipy_sparse(X).to_scipy_sparse()).sum(axis=0)
```

```{python}
gP = _sq(forest_pursuit_edge(X))
# Bp = sparse_adj_to_incidence()
# Bp
M = m.Contingent.from_scalar(gT,gP)

plt.plot(M.weights, M.G, color='k', alpha=0.3)

gP = _sq(expected_forest_maximization(X))
M = m.Contingent.from_scalar(gT,gP)

plt.plot(M.weights, M.G, color='k', alpha=0.6)


# np.diag(_sq(gP).sum(axis=0))
# edge_weights_to_laplacian(gP)
```

```{python}
# sns.histplot(gP, log_scale=True)
# hinton(_sq(gP))
# gP.shape
```

```{python}
 n1, n2 = np.triu(_sq(gP)).nonzero()
    # print(n1.shape)
E_obs = sparse.COO.from_scipy_sparse(_spanning_forests_obs_bootstrap(X))

e = np.ma.nonzero(gP)[0]
B = coo_array((np.concatenate([gP, -gP]), (np.concatenate([e,e]),np.concatenate([n1,n2]))), shape=(e.shape[0], X.shape[1]))
deg_est=E_obs@sparse.COO(np.abs(B.todense()))
Xest = 1-sparse.pow((1-X.sum(axis=0)/X.shape[0]), deg_est)
# print(deg_est)
print((lambda a: np.round(np.sqrt(sparse.diagonal(a.T@a).sum()),5))(sparse.COO.from_scipy_sparse(X) - Xest))
sns.histplot(np.ma.masked_less(gP, 0.2))
```

```{python}
# dists = bilinear_dists(sinkhorn(asc.ochiai(Xest)))
plt.spy(_sq(gP>0.1), marker='s', markersize=2, color='r')
plt.spy(_sq(gT), marker='+', alpha=0.7, markersize=3)
```

```{python}
from scipy.sparse.csgraph import shortest_path
true_dists = shortest_path(exp.graph.to_array())
true_dists
gP = _sq(forest_pursuit_edge(X, prior_dists=true_dists))
# Bp = sparse_adj_to_incidence()
# Bp
M = m.Contingent.from_scalar(gT,gP)

plt.plot(M.weights, M.G, color='k', alpha=0.3)
```

```{python}
from scipy.stats.distributions import bernoulli, beta
# N_activations = groupby_col0(sparse.COO.from_scipy_sparse(X).coords.T)
# E_activations = [_pursue_tree_basis(dists, N) for N in N_activations]
# E_coords = (
#     np.repeat(np.arange(len(N_activations)), np.array([len(e) for e in E_activations])),
#     np.concatenate(E_activations)
# )
# n = X.shape[1]
# m = n*(n-1)//2
# sparse.COO(E_coords, data=1, shape=(X.shape[0], m)).to_scipy_sparse(), X


# Xp = _spanning_forests_obs_bootstrap(X)
# Xp = X.copy()
# Xp.sum(axis=0)
# X
# yappi.stop()
# yappi.get_clock_time()-t1
# yappi.sort("ttot")
# yappi.get_func_stats().print_all()
# yappi.get_func_stats()[0]

och_dists = -np.log(asc.ochiai(X))
Lsym = edge_weights_to_laplacian(_sq(och_dists))
dists = och_dists
co_oc = _sq((X.T@X).toarray())
# beta = X.sum(axis=1).mean()**2/X.shape[1]**2
beta = .001
hist = []
# cts = np.zeros_like(co_oc)
gP = _sq(forest_pursuit_edge(X))
print(gP.shape, co_oc.shape)
diff=1.
i = 1

while diff>1e-5 and i<21:
    i+=1
    # gP = _sq(forest_pursuit_edge(Xp, prior_dists=dists))
    
    # cts = _sq(forest_pursuit_cts(X, prior_dists=dists))
    E_obs = sparse.COO.from_scipy_sparse(_spanning_forests_obs_bootstrap(X, prior_dists=dists))
    cts = E_obs.sum(axis=0)
    gP = gP + (cts - gP*co_oc)/(co_oc+1)
    # gP = pseudocount('min-connect')(cts,co_oc*(i+1))
    
    # Bp = sparse_adj_to_incidence()
    # Bp
    M = m.Contingent.from_scalar(gT,gP)

    plt.plot(M.weights, M.mcc, color=f'{max(1-(i+1)/20,0)}', label=f'{i}')
    # sns.histplot(gP[gP>0.8], color=f'{1-(i+1)/20}', label=f'{i}')


    
    n1, n2 = np.triu(_sq(gP)).nonzero()
    # print(n1.shape)
    e = np.ma.nonzero(gP)[0]
    B = coo_array((np.concatenate([gP, -gP]), (np.concatenate([e,e]),np.concatenate([n1,n2]))), shape=(e.shape[0], X.shape[1]))
    # if 'Lsym' in locals(): 
    Lold = Lsym
    
    # hist+=[np.ma.masked_less(np.abs(_sq(Lsym)),0.2)]
    hist+=[np.ma.masked_less(gP, 0.2)]
    L = B.T@B
    # print(np.allclose(L.sum(axis=0),0))
    # Lnew =  _norm_diag(L.toarray())
    # Lsym = Lold + 0.5*(Lnew-Lold)/2.
    Lsym = _norm_diag(L.toarray())
    # if 'Lold' in locals(): 
        
    # diff = np.max(np.abs(Lsym - Lold))
    diff = (lambda a: np.sqrt(np.trace(a.T@a)))(Lsym-Lold)
        # print(diff)
    print(np.round(trapezoid(M.mcc, M.weights),5), np.round(diff,5))

    
    # np.diag((B.T@B).toarray())==np.diag(nx.laplacian_matrix(G).toarray()).round(1)
    
    # E_obs = _spanning_forests_obs_bootstrap(X, prior_dists=dists)
    deg_est=E_obs@(sparse.COO(bernoulli(np.abs(B.todense())).rvs()))
    Xest = 1-sparse.pow((1-X.sum(axis=0)/X.shape[0]), deg_est)
    # print(deg_est)
    print((lambda a: np.round(np.sqrt(sparse.diagonal(a.T@a).sum()),5))(sparse.COO.from_scipy_sparse(X) - Xest))
    # dists = bilinear_dists(sinkhorn(asc.ochiai(Xest)))    
    # dists = adjusted_forest_dists(Lsym, beta=0.001)
    dists = generalized_graph_dists(Lsym, beta=beta)
    # dists = shortest_path(_sq(1/_sq(-Lsym)))
    # dists = (och_dists + generalized_graph_dists(Lsym, beta=beta))/2.

    # old_dists = dists
    # new_dists = generalized_graph_dists(Lsym, beta=beta)
    # dists = old_dists + 0.1*(new_dists-old_dists)/2
    
    ### WHEN <1 or >1??? think...

    # dists = shortest_path(-_sq(_sq((B.T@B).toarray())))

# from affinis.plots import hinton

# Xest
plt.legend()
```

```{python}
all_hists = np.ma.vstack(hist)
all_hists[:,all_hists.filled(0).max(axis=0)>0.2].shape
# all_hists.max(axis=0).
```

```{python}
plt.plot(all_hists[:,all_hists.filled(0).max(axis=0)>0.6], color='grey', alpha=0.2);
# hist[0]
```

```{python}
# plt.spy(Xest.to_scipy_sparse(), markersize=0.2)
# sns.heatmap(Xest.todense())
sns.histplot(Xest.data)
```

```{python}
# dists = bilinear_dists(sinkhorn(asc.ochiai(Xest)))
plt.spy(_sq(gP>0.1), marker='s', markersize=2, color='r')
plt.spy(_sq(gT), marker='+', alpha=0.7, markersize=2)
```

```{python}
sns.heatmap(generalized_graph_dists(Lsym, beta=0.001))
```

```{python}
gP = _sq(asc.forest_pursuit_edge(X))
# gP = _sq(asc.resource_project(X))
# gP = _sq(asc.ochiai(X))
# gP
# X
M = m.Contingent.from_scalar(gT,gP)

plt.plot(M.weights, M.mcc)

gP2 = _sq(asc.forest_pursuit_edge(X,prior_dists=dists))
M2 = m.Contingent.from_scalar(gT, gP2)
plt.plot(M2.weights, M2.mcc)

# M.mcc
```

```{python}
from scipy.stats import binned_statistic

plt.plot(M.weights, M.mcc)
binned = binned_statistic(M.weights, M.mcc, bins=50)
plt.plot((binned.bin_edges[1:]+binned.bin_edges[:-1])/2, binned.statistic)
plt.scatter(M.weights, M.weights)
plt.scatter(binned.bin_edges[:-1], binned.bin_edges[:-1])
```

```{python}
# plt.plot(M.weights, np.ma.log10(M.y_pred.sum(axis=1)/M.y_true.sum())))
sns.histplot(binned_statistic(M.weights, np.abs(M.y_pred.mean(axis=1)-M.y_true.mean()), bins=100).statistic,
             stat='density',)
sns.histplot(binned_statistic(M.weights, M.mcc, bins=100).statistic,
             stat='density',)
```

```{python}
plt.scatter(M.F, (M.mcc+1)/2)
plt.scatter(binned_statistic(M.mcc, M.F, bins=100).statistic, binned_statistic(M.mcc, (M.mcc+1)/2, bins=100).statistic, marker='.')
# np.ma.masked_invalid(binned_statistic(M.weights, (M.mcc+1)/2, bins=40).statistic)
# M.weights
mcc_f1_dists = 1-np.sqrt((M.F-1)**2+((M.mcc+1)/2 -1)**2)/np.sqrt(2)
binned_statistic(
    np.split(M.mcc, [np.argmax(M.mcc)])[0], 
    np.split(
        mcc_f1_dists, 
        [np.argmax(M.mcc)]
    )[0])
```

```{python}
def mcc_f1(M):
    # mcc_f1_dists = 1-np.sqrt((M.F-1)**2+(M.mcc -1)**2)/np.sqrt(2)
    mcc_f1_dists = 1-np.sqrt((M.F-1)**2+((M.mcc+1)/2 -1)**2)/np.sqrt(2)

    #need to subdivide further by binning separately on L and R of M.mcc.argmax()....
    argmax = np.argmax(M.mcc)
    x_L, x_R = np.split((M.mcc+1)/2., [argmax])
    # x_L, x_R = np.split(M.weights, [argmax])

    d_L, d_R = np.split(mcc_f1_dists, [argmax])
    
    sub_L=np.ma.masked_invalid(
        binned_statistic(
            x_L, 
            d_L, bins=100
        ).statistic).mean()
    sub_R=np.ma.masked_invalid(
        binned_statistic(
            x_R, 
            d_R, bins=100
        ).statistic).mean()
    
    return (sub_L+sub_R)/2.
mcc_f1(M)
# m.Contingent
```

```{python}
start = time.time()
_spanning_forests_obs_bootstrap(X).sum(axis=0)
print(time.time()-start)
%timeit _spanning_forests_obs_bootstrap(X).sum(axis=0)
```

```{python}
# X.tocsr()
# DVCFileSystem().read_text(_datasets[list(_datasets.keys())[0]])
# )

from scipy.integrate import trapezoid
# gP = _sq(asc.resource_project(X))
# gP = _sq((Snk.T@Snk)[:n,:n])
# gP = _sq(asc.SFD_edge_cond_prob(X))
import matplotlib as mpl

def compare_for_dataset(dataset_id, legend=True):
    exp = load_graph(dataset_id)
    X = csr_array(exp.activations.to_array().to_scipy_sparse())
    gT = _sq(exp.graph.to_array().todense()).astype(bool)
    
    
    
    f = mpl.figure.Figure(figsize=(8, 2))
    sf1, sf2, sf3 = f.subplots(1, 3, sharey=True)

    def handle_algs(algs,sf):
        for est in algs:
            gP = _estimators[est](X)
            M = m.Contingent.from_scalar(gT,gP)
            print(est,':\t',f'{trapezoid((M.mcc+1)/2., M.weights):.3f}',f'{np.sum(np.diff(M.recall[::-1], prepend=0) * M.precision[::-1]):.3f}')

            # print(est,':\t',f'{mcc_f1(M):.3f}',f'{trapezoid(np.abs(np.ma.log(M.y_pred.sum(axis=1)/M.y_true.sum())), M.weights):.3f}')
            # sf.plot(M.weights, np.abs(np.ma.log(M.y_pred.sum(axis=1)/M.y_true.sum())), label=est)
            # sf.scatter(M.F, (M.mcc+1)/2, label=est, marker='.')
            # sf.plot(M.weights, (1-np.abs(M.y_pred.sum(axis=1)-M.y_true.sum())/M.y_true.sum()), label=est)
            # sf.plot(M.weights, 1-np.abs(M.y_pred.mean(axis=1)-M.y_true.mean()), label=est)
            sf.plot(M.weights, M.mcc, label=est)

        # sf1.legend(**legend_kws)
    
    handle_algs(['FP','FPi','GL'], sf1)  # 'EFM'
    handle_algs(['CoOc','CS','MI'], sf2)
    handle_algs(['eOT','HSS', 'RP'], sf3)
    # sf3.legend(**legend_kws)

    if legend:
        legend_kws = dict(loc='upper center', bbox_to_anchor=(0.5, -0.15),ncol=2)
        for sf in (sf1, sf2, sf3): 
            sf.legend(**legend_kws)
    f.suptitle(dataset_id+f' - {X.shape[0]} walks')
    # plt.tight_layout()
    return f
# X.T/X.sum(axis=1), X/X.sum(axis=0)
# sns.heatmap(np.diag(1/X.sum(axis=0))@X.T@np.diag(1/X.sum(axis=1))@X)
```

```{python}
compare_for_dataset(list(_datasets)[30])
```

```{python}
compare_for_dataset(list(_datasets)[210], legend=False)
```

```{python}
def sigfigs(n,sig):
    # return '{:g}'.format(float('{:.{p}g}'.format(n, p=sig)))
    return float('{:.{p}g}'.format(n, p=sig))
    

sigfigs(1.24412354651234, 4)
```

```{python}
# list(_datasets)[120:200]
```

```{python}
# import warnings
# warnings.simplefilter('ignore', 'RuntimeWarning')
yappi.clear_stats()
yappi.set_clock_type("wall")
algs = ['FP','FPi','CoOc','CS','MI','eOT','HSS', 'GL', 'RP']
# metrics = [m.F1, m.fowlkes_mallows, m.matthews_corrcoef]  # turn into registry? 
metrics=['F1','F-M','MCC','APS']

def process_experiment(expID, algs, metrics):
    exp = load_graph(expID)
    X = csr_array(exp.activations.to_array().to_scipy_sparse())
    gT = _sq(exp.graph.to_array().todense()).astype(bool)
    
    node_cts = X.sum(axis=0)
    actv_cts = X.sum(axis=1)
    
    res=dict()
    res['ID']=expID
    res['kind']=expID[:2]
    # res['thres-vals'] = M.weights
    res['n-edges']=gT.sum()
    res['n-nodes']=exp.graph.shape[0]
    res['n-walks']=exp.jumps.shape[0]
    res['n-jumps']=exp.jumps.shape[1]
    
    res['med-node-ct'] = np.median(node_cts)
    res['iqr-node-ct'] = iqr(node_cts)
    res['med-actv-ct'] = np.median(actv_cts)
    res['iqr-actv-ct'] = iqr(actv_cts)
    
    # res['scores'] = []
    for est in algs:
        method = dict()
        method['name']=est
        # yappi.clear_stats()
        # yappi.start()
        # start=yappi.get_clock_time()
        start = time.time()
        gP = _estimators[est](X)
        end = time.time()
        # end = yappi.get_clock_time()
        # yappi.stop()
        method['seconds']= sigfigs(end - start,5)
        M = m.Contingent.from_scalar(gT, gP)
            
        scores = {met:sigfigs(_metrics[met](M),5) for met in metrics}
            # continue
        # for metric in metrics:
            # y=metric(M)
            # res[metric.__name__] = y
            # res[metric.__name__] = trapezoid(y, x=M.weights)
            # res[f'E[{metric.__name__}]'] = trapezoid(y, x=M.weights)
        # yield res
        # res['scores'] += [method | scores]
        yield res | method | scores


# # list()

# ar=ak.Array(chain(*(process_experiment(g, algs, metrics) for g in list(_datasets)[:10])))
# pd.json_normalize(ak.to_json(ar))
# ak.to_dataframe(ar, how='outer')
# ak.flatten(ar)
# ar
# ak.zip(ak.unzip(ar), depth_limit=1)

# ak.to_dataframe(ar)
# df=pd.DataFrame(tqdm(chain(*(process_experiment(g, algs, metrics) for g in list(_datasets)[120:130]))))
# df
```

```{python}
# df.to_csv('res2-short.csv')
# temp-df = pd.read_csv('res.csv', index_col=0)
```

```{python}
# res
```

```{python}
gen = chain(*(process_experiment(exp, algs, metrics) 
              for exp in list(_datasets)))
tot_exp = len(algs)*len(_datasets)
res = pd.DataFrame(tqdm(
    gen,
    # process_experiment('BL-N100S04',algs, metrics), 
    total=tot_exp
))
res.to_csv('res.csv')

 
# df.to_csv('res')
# ak.to_json(ar, 'res.json')
```

```{python}
res.to_csv('res.csv')
res
```

```{python}
gen = chain(*(process_experiment(exp, ['GL'], metrics) 
              for exp in list(_datasets)[:20]))
tot_exp = len(algs)*len(_datasets)#*len(metrics)#
# df = pd.DataFrame(tqdm(process_experiment('BL-N100S04',algs, metrics), total=tot_exp)
# df.head()
# import awkward as ak
ar = ak.from_iter(tqdm(
    gen,
    # process_experiment('BL-N100S04',algs, metrics), 
    total=tot_exp
))
# ak.to_json(ar,'res.json')
# import polars as pl
ar
```

```{python}
f, ax = plt.subplots(ncols = 3,nrows=2, figsize=(8,3), sharey=True, sharex=True)
ax[0,0].set_ylabel('score')
ax[0,0].set_ylim(0,1)
ax[1,0].set_ylabel('cuml. score')

fr,axr=plt.subplots(nrows=2, sharey=True,sharex=True, figsize=(3,5))
axr[0].set_ylim(0,1)
for est in :
    gP = _estimators[est](X, pseudocts='min-connect')
    print(f'{est}\t {average_precision_score(gT, gP):0.2f}')
    M = m.Contingent.from_scalar(
        gT,
        # norm.cdf(zscore(gP))
        gP
    )
    
    for n,metric in enumerate(metrics):
        x,y = M.weights, metric(M)
        print(f'\t {metric.__name__}\t{trapezoid(y, x=x):0.2f}')
        ax[0,n].plot(x, y, label=est)

    axr[0].step(M.recall, M.precision, label=est, marker='|', where='post')
    # axr[1].step(M.recall[::-1],cumulative_trapezoid(M.precision[::-1], x=M.recall[::-1], initial=0))
    axr[1].plot(M.recall[::-1],np.cumsum(np.diff(M.recall[::-1], prepend=0.)*M.precision[::-1]))
handles, labels = ax[0,0].get_legend_handles_labels()
lgd = f.legend(handles, labels, loc='upper center', bbox_to_anchor=(1.05,0.8))
f.tight_layout()
```

```{python}
plt.figure(figsize=(5,5))
plt.step(M.recall, M.precision, color='k', ls='--')

contour_Fscore()

# der contour_
    
plt.ylim(0,1.1)
plt.xlim(0,1.1)
sns.despine()
print(-trapezoid(M.precision, x=M.recall))
```

```{python}
[(e.jumps.shape, e.graph.to_array().shape[0]) for e in map(load_graph, _datasets.keys())]
```

```{python}
# from functools import partial
```
