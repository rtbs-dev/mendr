---
jupyter:
  jupytext:
    formats: 'ipynb,qmd,md:myst'
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.0
  kernelspec:
    display_name: mendr
    language: python
    name: mendr
---

```{python}
import numpy as np
import mendr.metrics as m
from scipy.stats import ecdf
from scipy.integrate import trapezoid, simpson, cumulative_trapezoid
import time
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
# import seaborn as sns
```

```{python}
rng = np.random.default_rng(2)

X = rng.choice(rng.beta(0.3, 0.7, size=10), size=25)
y = rng.binomial(1,X).astype(bool)

Y = m.Contingent.from_scalar(y, X)
T = Y.weights

# T = m.gen_thres_vals(X)
# E = ecdf(X)
# T = np.pad(E.cdf.probabilities, ((1,1)),constant_values=(0,1))
# Xt = m._all_thres(E.cdf.evaluate(X), T).astype(bool)

# Yt = m.Contingent(y, m._all_thres(X, np.unique(X)))

# d = percentileofscore(X, T)/100.
# d = ecdf(X).sf.evaluate(T)
# d = ecdf(X).cdf.evaluate(T)
# Y
```

```{python}
plt.plot(T)
```

```{python}
plt.hist(X);
```

```{python}
np.round(ecdf(X).cdf.evaluate(X),2)
```

```{python}
# X.toarray().sum(axis=0).mean()
t ~ O(k*a*b)
```

```{python}
y.astype(int)
```

```{python}
# plt.hist(X, density=True)
# plt.spy(Xt)
# Xt.astype(int)
Y.F
```

```{python}
plt.plot(T,Y.F)
# plt.plot(d,Y.F)
plt.axvline(1-y.mean(), color='k', ls='--')
simpson(m.F1(Y), x=T), m.F1(Y).mean()#, -simpson(m.F1(Y), x=d)
```

```{python}
plt.plot(T,Y.mcc)
# plt.plot(d,Y.mcc)
plt.axvline(1-y.mean(), color='k', ls='--')
simpson(m.matthews_corrcoef(Y), x=T), m.matthews_corrcoef(Y).mean()#, -simpson(m.matthews_corrcoef(Y), x=d)
```

```{python}
plt.plot(T,m.fowlkes_mallows(Y))

# plt.plot(d,m.fowlkes_mallows(Y))
plt.axvline(1-y.mean(), color='k', ls='--')
trapezoid(m.fowlkes_mallows(Y), x=T), m.fowlkes_mallows(Y).mean(), #-simpson(m.fowlkes_mallows(Y), x=d)
```

```{python}
plt.figure(figsize=(5,5))
plt.step(Y.recall, Y.precision, color='k', ls='--', where='post')

def contour_Fscore():
    f_scores = np.linspace(0.2, 0.8, num=4)
    lines, labels = [], []
    for f_score in f_scores:
        x = np.linspace(0.0001, 1.)

        y_f1 = f_score * x / (2 * x - f_score)
        y_fm = f_score**2/x

        (l,) = plt.plot(x[0<=y_f1], y_f1[0<=y_f1], color="xkcd:orange", alpha=0.2)
        (l,) = plt.plot(x[0<=y_fm], y_fm[y_fm >= 0], color="xkcd:sky", alpha=0.2)
        # midpt = y_fm[25]-0.03
        plt.annotate("fm={0:0.1f}".format(f_score), xy=(y_f1[25]+0.01, y_f1[25]+0.1), color='xkcd:slate')
        # plt.annotate("f1={0:0.1f}".format(f_score), xy=(1.1, y_f1[48]-0.01), color='xkcd:orange')


contour_Fscore()

# der contour_
plt.plot(Y.recall[::-1], cumulative_trapezoid(Y.precision[::-1], x=Y.recall[::-1], initial=0.))
plt.plot(Y.recall[::-1], np.cumsum(np.diff(Y.recall[::-1], prepend=0)*Y.precision[::-1]))

plt.ylim(0,1.1)
plt.xlim(0,1.1)
sns.despine()
# np.cumsum(Y.precision[::-1]*np.diff(Y.recall[::-1], prepend=0))
# Y.recall
```

```{python}
plt.plot(np.cumsum(np.diff(Y.recall[::-1], prepend=0)*Y.precision[::-1]))
```

```{python}
plt.plot(Y.recall,cumulative_trapezoid(Y.precision, x=Y.recall, initial=0,))
```

```{python}
from mendr.experiments import (
    load_graph, _datasets,_estimators, _metrics,_sq, _dataset_paths
)
from mendr.metrics import minmax_tf
from dvc.api import DVCFileSystem
from scipy.special import expit
from scipy.stats import zscore, norm, beta, iqr
from scipy.sparse import csr_array
from sklearn.metrics import average_precision_score
import re
from affinis import associations as asc
from itertools import chain
from tqdm.auto import tqdm
import pandas as pd
import awkward as ak

from sklearn.covariance import graphical_lasso
# !cd /home/rtbsexton/Documents/code/graph-recovery-dataset/docs
# DVCFileSystem().find("data/", dvc_only=True, detail=False)
# exp = load_graph( list(_datasets.keys())[-1])
exp = load_graph(list(_datasets)[100])
# np.exp(-asc.mutual_information(csr_array(exp.activations.to_array().to_scipy_sparse())))
# exp.activations.to_array().sum(axis=1).todense()
# [exp.jumps.shape, exp.graph.to_array()
# _datasets
# _dataset_paths
# _estimators
# [load_graph(i) for i in list(_datasets.keys())[:10]] 
X = csr_array(exp.activations.to_array().to_scipy_sparse())
gT = _sq(exp.graph.to_array().todense()).astype(bool)
```

```{python}

exp.activations.to_array().sum(axis=1).mean(), exp.jumps.shape[1]
```

```{python}
from affinis.proximity import sinkhorn
from affinis.priors import pseudocount
# plt.imshow(sinkhorn(X).todense())
w,n=X.shape
BA = np.block(
    [[np.zeros((n,n)),X.todense().T],
     [X.todense(),np.zeros((w,w))]]
)
Snk = sinkhorn(BA)
plt.imshow((Snk.T@Snk)[:n,:n])
```

```{python}
# pseudocount(0.5)(X, X.sum(axis=0))

# np.dot(X.sum(axis=0), X.sum(axis=1))
# (X.T@X).todense()/np.multiply.outer(X.sum(axis=0), np.ones(X.shape[1]))
# D = np.diag(1/X.sum(axis=0))
# (1-D@(X.T) == (X/X.sum(axis=0)).T).sum()

BA.shape
```

```{python}
# X.tocsr()
# DVCFileSystem().read_text(_datasets[list(_datasets.keys())[0]])
# )

# gP = _sq(asc.resource_project(X))
# gP = _sq((Snk.T@Snk)[:n,:n])
# gP = _sq(asc.SFD_edge_cond_prob(X))
gP = _sq(-graphical_lasso(asc.coocur_prob(X), 0.01)[1])
# gP = _sq(asc.SFD_edge_cond_prob(X, pseudocts='min-connect'))
# plt.hist(gP)
M = m.Contingent.from_scalar(gT,gP)
plt.plot(M.weights, M.mcc)
# sns.distplot(_sq(asc.resource_project(X)))
# X.T/X.sum(axis=1), X/X.sum(axis=0)
# sns.heatmap(np.diag(1/X.sum(axis=0))@X.T@np.diag(1/X.sum(axis=1))@X)
```

```{python}
def sigfigs(n,sig):
    # return '{:g}'.format(float('{:.{p}g}'.format(n, p=sig)))
    return float('{:.{p}g}'.format(n, p=sig))
    

sigfigs(1.24412354651234, 4)
```

```{python}
# import warnings
# warnings.simplefilter('ignore', 'RuntimeWarning')

algs = ['TS','TSi','CS','MI','eOT','HSS', 'GL', 'RP']
# metrics = [m.F1, m.fowlkes_mallows, m.matthews_corrcoef]  # turn into registry? 
metrics=['F1','F-M','MCC','APS']

def process_experiment(expID, algs, metrics):
    exp = load_graph(expID)
    X = csr_array(exp.activations.to_array().to_scipy_sparse())
    gT = _sq(exp.graph.to_array().todense()).astype(bool)
    
    node_cts = X.sum(axis=0)
    actv_cts = X.sum(axis=1)
    
    res=dict()
    res['ID']=expID
    res['kind']=expID[:2]
    # res['thres-vals'] = M.weights

    res['n-nodes']=exp.graph.shape[0]
    res['n-walks']=exp.jumps.shape[0]
    res['n-jumps']=exp.jumps.shape[1]
    res['med-node-ct'] = np.median(node_cts)
    res['iqr-node-ct'] = iqr(node_cts)
    res['med-actv-ct'] = np.median(actv_cts)
    res['iqr-actv-ct'] = iqr(actv_cts)
    
    # res['scores'] = []
    for est in algs:
        method = dict()
        method['name']=est
        start = time.time()
        gP = _estimators[est](X)
        end = time.time()
        method['seconds']= sigfigs(end - start,5)
        M = m.Contingent.from_scalar(gT, gP)
            
        scores = {met:sigfigs(_metrics[met](M),5) for met in metrics}
            # continue
        # for metric in metrics:
            # y=metric(M)
            # res[metric.__name__] = y
            # res[metric.__name__] = trapezoid(y, x=M.weights)
            # res[f'E[{metric.__name__}]'] = trapezoid(y, x=M.weights)
        # yield res
        # res['scores'] += [method | scores]
        yield res | method | scores


# # list()

# ar=ak.Array(chain(*(process_experiment(g, algs, metrics) for g in list(_datasets)[:10])))
# pd.json_normalize(ak.to_json(ar))
# ak.to_dataframe(ar, how='outer')
# ak.flatten(ar)
# ar
# ak.zip(ak.unzip(ar), depth_limit=1)

# ak.to_dataframe(ar)
df=pd.DataFrame(tqdm(chain(*(process_experiment(g, algs, metrics) for g in list(_datasets)[:50]))))
# df
```

```{python}
df.to_csv('res2-short.csv')
```

```{python}
gen = chain(*(process_experiment(exp, algs, metrics) 
              for exp in list(_datasets)))
tot_exp = len(algs)*len(_datasets)
res = pd.DataFrame(tqdm(
    gen,
    # process_experiment('BL-N100S04',algs, metrics), 
    total=tot_exp
))
res.to_csv('res2.csv')

 
# df.to_csv('res')
# ak.to_json(ar, 'res.json')
```

```{python}
# res.to_csv('res.csv')
res
```

```{python}
gen = chain(*(process_experiment(exp, ['GL'], metrics) 
              for exp in list(_datasets)[:20]))
tot_exp = len(algs)*len(_datasets)#*len(metrics)#
# df = pd.DataFrame(tqdm(process_experiment('BL-N100S04',algs, metrics), total=tot_exp)
# df.head()
# import awkward as ak
ar = ak.from_iter(tqdm(
    gen,
    # process_experiment('BL-N100S04',algs, metrics), 
    total=tot_exp
))
# ak.to_json(ar,'res.json')
# import polars as pl
ar
```

```{python}
f, ax = plt.subplots(ncols = 3,nrows=2, figsize=(8,3), sharey=True, sharex=True)
ax[0,0].set_ylabel('score')
ax[0,0].set_ylim(0,1)
ax[1,0].set_ylabel('cuml. score')

fr,axr=plt.subplots(nrows=2, sharey=True,sharex=True, figsize=(3,5))
axr[0].set_ylim(0,1)
for est in :
    gP = _estimators[est](X, pseudocts='min-connect')
    print(f'{est}\t {average_precision_score(gT, gP):0.2f}')
    M = m.Contingent.from_scalar(
        gT,
        # norm.cdf(zscore(gP))
        gP
    )
    
    for n,metric in enumerate(metrics):
        x,y = M.weights, metric(M)
        print(f'\t {metric.__name__}\t{trapezoid(y, x=x):0.2f}')
        ax[0,n].plot(x, y, label=est)

    axr[0].step(M.recall, M.precision, label=est, marker='|', where='post')
    # axr[1].step(M.recall[::-1],cumulative_trapezoid(M.precision[::-1], x=M.recall[::-1], initial=0))
    axr[1].plot(M.recall[::-1],np.cumsum(np.diff(M.recall[::-1], prepend=0.)*M.precision[::-1]))
handles, labels = ax[0,0].get_legend_handles_labels()
lgd = f.legend(handles, labels, loc='upper center', bbox_to_anchor=(1.05,0.8))
f.tight_layout()
```

```{python}
plt.figure(figsize=(5,5))
plt.step(M.recall, M.precision, color='k', ls='--')

contour_Fscore()

# der contour_
    
plt.ylim(0,1.1)
plt.xlim(0,1.1)
sns.despine()
print(-trapezoid(M.precision, x=M.recall))
```

```{python}
[(e.jumps.shape, e.graph.to_array().shape[0]) for e in map(load_graph, _datasets.keys())]
```
